{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (f1_score, roc_auc_score, accuracy_score, balanced_accuracy_score,\n",
    "                            precision_score, recall_score, roc_curve, auc, confusion_matrix, classification_report)\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras\n",
    "import pickle\n",
    "import gc\n",
    "from tensorflow.keras import mixed_precision\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "import json\n",
    "from optuna.samplers import NSGAIISampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T05:06:26.486594Z",
     "iopub.status.busy": "2025-07-03T05:06:26.486347Z",
     "iopub.status.idle": "2025-07-03T05:06:26.507231Z",
     "shell.execute_reply": "2025-07-03T05:06:26.506604Z",
     "shell.execute_reply.started": "2025-07-03T05:06:26.486579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(gpus, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T05:06:26.524545Z",
     "iopub.status.busy": "2025-07-03T05:06:26.524355Z",
     "iopub.status.idle": "2025-07-03T05:06:26.541754Z",
     "shell.execute_reply": "2025-07-03T05:06:26.541181Z",
     "shell.execute_reply.started": "2025-07-03T05:06:26.524531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\"Configuration class for model hyperparameters\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.img_height = kwargs.get(\"img_height\", 512)\n",
    "        self.img_width = kwargs.get(\"img_width\", 512)\n",
    "        self.channels = kwargs.get(\"channels\", 1)  # Grayscale\n",
    "        self.num_classes = kwargs.get(\"num_classes\", 4)\n",
    "        self.batch_size = kwargs.get(\"batch_size\", 16)\n",
    "        self.epochs = kwargs.get(\"epochs\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T05:06:26.638319Z",
     "iopub.status.busy": "2025-07-03T05:06:26.637984Z",
     "iopub.status.idle": "2025-07-03T05:06:26.671229Z",
     "shell.execute_reply": "2025-07-03T05:06:26.670527Z",
     "shell.execute_reply.started": "2025-07-03T05:06:26.638295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Handle data processing and augmentation for MRI images.\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def create_kfold_data(self, base_dir: str, n_splits: int = 5, seed: int = 42):\n",
    "        \"\"\"Yields tf.data.Dataset objects for training and validation for each fold.\"\"\"\n",
    "        df, self.class_indices = self._load_image_paths_and_labels(base_dir)\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        class_names = list(self.class_indices.keys())\n",
    "        num_classes = len(class_names)\n",
    "\n",
    "        def augment_image(image, label):\n",
    "            image = tf.image.random_flip_left_right(image, seed=seed)\n",
    "            angle = tf.random.uniform([], -15 * np.pi / 180, 15 * np.pi / 180, dtype=tf.float32)\n",
    "            image = tf.image.rot90(image, k=tf.cast(angle * 4 / (2 * np.pi), tf.int32))\n",
    "            image = tf.image.random_brightness(image, max_delta=0.1, seed=seed)\n",
    "            image = tf.image.random_contrast(image, lower=0.9, upper=1.1, seed=seed)\n",
    "            scale = tf.random.uniform([], 0.9, 1.1, dtype=tf.float32)\n",
    "            new_height = tf.cast(tf.cast(self.config.img_height, tf.float32) * scale, tf.int32)\n",
    "            new_width = tf.cast(tf.cast(self.config.img_width, tf.float32) * scale, tf.int32)\n",
    "            image = tf.image.resize(image, [new_height, new_width])\n",
    "            image = tf.image.resize_with_crop_or_pad(image, self.config.img_height, self.config.img_width)\n",
    "            # Shear transformation\n",
    "            shear = tf.random.uniform([], -0.1, 0.1, dtype=tf.float32)\n",
    "            # Construct a 3x3 projective transform matrix for shear\n",
    "            shear_matrix = tf.stack([\n",
    "                tf.constant(1.0), shear, tf.constant(0.0),  # [1, s, 0]\n",
    "                tf.constant(0.0), tf.constant(1.0), tf.constant(0.0),  # [0, 1, 0]\n",
    "                tf.constant(0.0), tf.constant(0.0), tf.constant(1.0)   # [0, 0, 1]\n",
    "            ])\n",
    "            # Flatten to [9], then take first 8 elements for projective transform\n",
    "            shear_matrix = tf.reshape(shear_matrix, [9])[:8]  # [1, s, 0, 0, 1, 0, 0, 0]\n",
    "            shear_matrix = tf.expand_dims(shear_matrix, 0)  # Shape: [1, 8]\n",
    "            image = tf.raw_ops.ImageProjectiveTransformV3(\n",
    "                images=tf.expand_dims(image, 0),\n",
    "                transforms=shear_matrix,\n",
    "                output_shape=[self.config.img_height, self.config.img_width],\n",
    "                fill_value=0.0,\n",
    "                interpolation='BILINEAR'\n",
    "            )[0]\n",
    "            noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.02, dtype=tf.float32)\n",
    "            image = image + noise\n",
    "            shift_fraction = 0.05\n",
    "            crop_height = int(self.config.img_height * (1 - 2 * shift_fraction))\n",
    "            crop_width = int(self.config.img_width * (1 - 2 * shift_fraction))\n",
    "            image = tf.image.random_crop(\n",
    "                image,\n",
    "                size=[crop_height, crop_width, self.config.channels],\n",
    "                seed=seed\n",
    "            )\n",
    "            max_offset_height = int(self.config.img_height * shift_fraction)\n",
    "            max_offset_width = int(self.config.img_width * shift_fraction)\n",
    "            offset_height = tf.random.uniform([], 0, max_offset_height, dtype=tf.int32)\n",
    "            offset_width = tf.random.uniform([], 0, max_offset_width, dtype=tf.int32)\n",
    "            image = tf.image.pad_to_bounding_box(\n",
    "                image,\n",
    "                offset_height=offset_height,\n",
    "                offset_width=offset_width,\n",
    "                target_height=self.config.img_height,\n",
    "                target_width=self.config.img_width\n",
    "            )\n",
    "            image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "            return image, label\n",
    "\n",
    "        def load_and_preprocess(path, label):\n",
    "            image = tf.io.read_file(path)\n",
    "            image = tf.image.decode_jpeg(image, channels=self.config.channels)\n",
    "            image = tf.image.resize(image, [self.config.img_height, self.config.img_width])\n",
    "            image = tf.py_function(self._normalize_image, [image], tf.float32)\n",
    "            image.set_shape([self.config.img_height, self.config.img_width, self.config.channels])\n",
    "            return image, tf.one_hot(label, num_classes)\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(df.filepath, df.label)):\n",
    "            print(f\"\\nðŸ“‚ Fold {fold + 1}/{n_splits}\")\n",
    "            train_paths = df.filepath.iloc[train_idx].values\n",
    "            train_labels = df.label.iloc[train_idx].values\n",
    "            val_paths = df.filepath.iloc[val_idx].values\n",
    "            val_labels = df.label.iloc[val_idx].values\n",
    "            train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "            train_ds = train_ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            train_ds = train_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            train_ds = train_ds.shuffle(1024).batch(self.config.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "            val_ds = val_ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            val_ds = val_ds.batch(self.config.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "            yield fold, train_ds, val_ds\n",
    "\n",
    "    def create_full_dataset(self, train_dir: str, validation_dir: str):\n",
    "        \"\"\"Create tf.data.Dataset for full training and validation sets.\"\"\"\n",
    "        train_df, class_indices = self._load_image_paths_and_labels(train_dir)\n",
    "        val_df, _ = self._load_image_paths_and_labels(validation_dir)\n",
    "        num_classes = len(class_indices)\n",
    "\n",
    "        def augment_image(image, label):\n",
    "            image = tf.image.random_flip_left_right(image, seed=42)\n",
    "            angle = tf.random.uniform([], -15 * np.pi / 180, 15 * np.pi / 180, dtype=tf.float32)\n",
    "            image = tf.image.rot90(image, k=tf.cast(angle * 4 / (2 * np.pi), tf.int32))\n",
    "            image = tf.image.random_brightness(image, max_delta=0.1, seed=42)\n",
    "            image = tf.image.random_contrast(image, lower=0.9, upper=1.1, seed=42)\n",
    "            scale = tf.random.uniform([], 0.9, 1.1, dtype=tf.float32)\n",
    "            new_height = tf.cast(tf.cast(self.config.img_height, tf.float32) * scale, tf.int32)\n",
    "            new_width = tf.cast(tf.cast(self.config.img_width, tf.float32) * scale, tf.int32)\n",
    "            image = tf.image.resize(image, [new_height, new_width])\n",
    "            image = tf.image.resize_with_crop_or_pad(image, self.config.img_height, self.config.img_width)\n",
    "            shear = tf.random.uniform([], -0.1, 0.1, dtype=tf.float32)\n",
    "            shear_matrix = tf.stack([\n",
    "                tf.constant(1.0), shear, tf.constant(0.0),\n",
    "                tf.constant(0.0), tf.constant(1.0), tf.constant(0.0),\n",
    "                tf.constant(0.0), tf.constant(0.0), tf.constant(1.0)\n",
    "            ])\n",
    "            shear_matrix = tf.reshape(shear_matrix, [9])[:8]\n",
    "            shear_matrix = tf.expand_dims(shear_matrix, 0)\n",
    "            image = tf.raw_ops.ImageProjectiveTransformV3(\n",
    "                images=tf.expand_dims(image, 0),\n",
    "                transforms=shear_matrix,\n",
    "                output_shape=[self.config.img_height, self.config.img_width],\n",
    "                fill_value=0.0,\n",
    "                interpolation='BILINEAR'\n",
    "            )[0]\n",
    "            noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.02, dtype=tf.float32)\n",
    "            image = image + noise\n",
    "            shift_fraction = 0.05\n",
    "            crop_height = int(self.config.img_height * (1 - 2 * shift_fraction))\n",
    "            crop_width = int(self.config.img_width * (1 - 2 * shift_fraction))\n",
    "            image = tf.image.random_crop(\n",
    "                image,\n",
    "                size=[crop_height, crop_width, self.config.channels],\n",
    "                seed=42\n",
    "            )\n",
    "            max_offset_height = int(self.config.img_height * shift_fraction)\n",
    "            max_offset_width = int(self.config.img_width * shift_fraction)\n",
    "            offset_height = tf.random.uniform([], 0, max_offset_height, dtype=tf.int32)\n",
    "            offset_width = tf.random.uniform([], 0, max_offset_width, dtype=tf.int32)\n",
    "            image = tf.image.pad_to_bounding_box(\n",
    "                image,\n",
    "                offset_height=offset_height,\n",
    "                offset_width=offset_width,\n",
    "                target_height=self.config.img_height,\n",
    "                target_width=self.config.img_width\n",
    "            )\n",
    "            image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "            return image, label\n",
    "\n",
    "        def load_and_preprocess(path, label):\n",
    "            image = tf.io.read_file(path)\n",
    "            image = tf.image.decode_jpeg(image, channels=self.config.channels)\n",
    "            image = tf.image.resize(image, [self.config.img_height, self.config.img_width])\n",
    "            image = tf.py_function(self._normalize_image, [image], tf.float32)\n",
    "            image.set_shape([self.config.img_height, self.config.img_width, self.config.channels])\n",
    "            return image, tf.one_hot(label, num_classes)\n",
    "\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((train_df.filepath, train_df.label))\n",
    "        train_ds = train_ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        train_ds = train_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        train_ds = train_ds.shuffle(1024).batch(self.config.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((val_df.filepath, val_df.label))\n",
    "        val_ds = val_ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        val_ds = val_ds.batch(self.config.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return train_ds, val_ds\n",
    "\n",
    "    def create_hardware_validation_dataset(self, base_dir: str):\n",
    "        \"\"\"Create tf.data.Dataset for validation on hardware Gabor-filtered dataset.\"\"\"\n",
    "        data = []\n",
    "        class_names = sorted(os.listdir(base_dir))\n",
    "        label_map = {'glioma': 0, 'meningioma': 1, 'notumor': 2}\n",
    "        for class_name in class_names:\n",
    "            class_path = os.path.join(base_dir, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "            for run_name in os.listdir(class_path):\n",
    "                run_path = os.path.join(class_path, run_name)\n",
    "                if not os.path.isdir(run_path):\n",
    "                    continue\n",
    "                run_images = sorted([f for f in os.listdir(run_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "                if len(run_images) != 4:\n",
    "                    print(f\"Warning: Run {run_path} has {len(run_images)} images, expected 4. Skipping.\")\n",
    "                    continue\n",
    "                run_image_paths = [os.path.join(run_path, img) for img in run_images]\n",
    "                data.append((run_image_paths, class_name))\n",
    "        \n",
    "        if not data:\n",
    "            raise ValueError(\"No valid runs found in hardware dataset directory\")\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\"image_paths\", \"class_name\"])\n",
    "        df[\"label\"] = df.class_name.map(label_map)\n",
    "        \n",
    "        def load_and_stack_images(image_paths, label):\n",
    "            images = []\n",
    "            for path in image_paths:\n",
    "                image = tf.io.read_file(path)\n",
    "                image = tf.image.decode_jpeg(image, channels=1)\n",
    "                image = tf.image.resize(image, [self.config.img_height, self.config.img_width])\n",
    "                image = tf.py_function(self._normalize_image, [image], tf.float32)\n",
    "                image.set_shape([self.config.img_height, self.config.img_width, 1])\n",
    "                images.append(image)\n",
    "            stacked_image = tf.concat(images, axis=-1)\n",
    "            return stacked_image, tf.one_hot(label, depth=4)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df.image_paths, df.label))\n",
    "        dataset = dataset.map(load_and_stack_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.batch(self.config.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset, class_names\n",
    "\n",
    "    def compute_class_weights_from_dataset(self, dataset):\n",
    "        \"\"\"Calculate class weights to handle imbalance from tf.data.Dataset.\"\"\"\n",
    "        labels = []\n",
    "        for _, label in dataset.unbatch():\n",
    "            labels.append(np.argmax(label.numpy()))\n",
    "        class_counts = np.bincount(labels)\n",
    "        total_samples = len(labels)\n",
    "        class_weights = {}\n",
    "        for cls, count in enumerate(class_counts):\n",
    "            weight = total_samples / (len(class_counts) * count)\n",
    "            if cls == 0:\n",
    "                weight *= 2.0\n",
    "            class_weights[cls] = weight\n",
    "        return class_weights\n",
    "\n",
    "    def _load_image_paths_and_labels(self, base_dir: str):\n",
    "        \"\"\"Return a DataFrame with filepaths and class labels.\"\"\"\n",
    "        data = []\n",
    "        for class_name in sorted(os.listdir(base_dir)):\n",
    "            class_path = os.path.join(base_dir, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "            for fname in os.listdir(class_path):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    data.append((os.path.join(class_path, fname), class_name))\n",
    "        df = pd.DataFrame(data, columns=[\"filepath\", \"class_name\"])\n",
    "        label_map = {name: idx for idx, name in enumerate(sorted(df.class_name.unique()))}\n",
    "        df[\"label\"] = df.class_name.map(label_map)\n",
    "        return df, label_map\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_image(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize image using z-score normalization.\"\"\"\n",
    "        return (x - np.mean(x)) / (np.maximum(np.std(x), 1e-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T05:06:26.672436Z",
     "iopub.status.busy": "2025-07-03T05:06:26.672185Z",
     "iopub.status.idle": "2025-07-03T05:06:26.695271Z",
     "shell.execute_reply": "2025-07-03T05:06:26.694649Z",
     "shell.execute_reply.started": "2025-07-03T05:06:26.672420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"Handles model evaluation and visualization.\"\"\"\n",
    "    def __init__(self, model, num_classes):\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def get_predictions_and_labels(self, data, class_indices=None):\n",
    "        \"\"\"Get predictions and labels from tf.data.Dataset, handling class subset if needed.\"\"\"\n",
    "        if isinstance(data, tf.data.Dataset):\n",
    "            images, labels = [], []\n",
    "            for batch_images, batch_labels in data.unbatch().batch(1):\n",
    "                images.append(batch_images.numpy())\n",
    "                labels.append(batch_labels.numpy())\n",
    "            images = np.concatenate(images, axis=0)\n",
    "            labels = np.concatenate(labels, axis=0)\n",
    "            predictions = self.model.predict(images)\n",
    "            y_true = np.argmax(labels, axis=1) if labels.ndim > 1 else labels\n",
    "            y_true_onehot = labels if labels.ndim > 1 else to_categorical(y_true, num_classes=self.num_classes)\n",
    "            # If class_indices is provided (e.g., [0, 1, 3]), adjust predictions and labels\n",
    "            if class_indices is not None:\n",
    "                # Extract predictions for the relevant classes\n",
    "                predictions = predictions[:, class_indices]\n",
    "                # Recompute y_true for the subset of classes\n",
    "                y_true_mapped = []\n",
    "                for label in y_true:\n",
    "                    # Map the original label to the new index in class_indices\n",
    "                    idx = class_indices.index(label) if label in class_indices else -1\n",
    "                    y_true_mapped.append(idx)\n",
    "                y_true_mapped = np.array(y_true_mapped)\n",
    "                # Filter out samples with unmapped labels (e.g., notumor)\n",
    "                valid_indices = y_true_mapped != -1\n",
    "                y_true_mapped = y_true_mapped[valid_indices]\n",
    "                predictions = predictions[valid_indices]\n",
    "                y_true_onehot = y_true_onehot[valid_indices][:, class_indices]\n",
    "            else:\n",
    "                y_true_mapped = y_true\n",
    "        else:\n",
    "            raise ValueError(\"Expected tf.data.Dataset\")\n",
    "        return predictions, y_true_mapped, y_true_onehot\n",
    "\n",
    "    def evaluate(self, data, class_indices=None):\n",
    "        \"\"\"Evaluate the model using key metrics, handling class subset if needed.\"\"\"\n",
    "        predictions, y_true, y_true_onehot = self.get_predictions_and_labels(data, class_indices)\n",
    "        # Skip evaluation if no valid samples after filtering\n",
    "        if len(y_true) == 0:\n",
    "            print(\"No valid samples to evaluate after filtering classes.\")\n",
    "            return {}\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(y_true, np.argmax(predictions, axis=1)),\n",
    "            'balanced_accuracy': balanced_accuracy_score(y_true, np.argmax(predictions, axis=1)),\n",
    "            'macro_precision': precision_score(y_true, np.argmax(predictions, axis=1), average='macro'),\n",
    "            'macro_recall': recall_score(y_true, np.argmax(predictions, axis=1), average='macro'),\n",
    "            'macro_f1': f1_score(y_true, np.argmax(predictions, axis=1), average='macro'),\n",
    "            'macro_roc_auc': roc_auc_score(y_true_onehot, predictions, average='macro', multi_class='ovr')\n",
    "        }\n",
    "        num_classes = len(class_indices) if class_indices is not None else self.num_classes\n",
    "        for i in range(num_classes):\n",
    "            results[f'class_{i}_roc_auc'] = roc_auc_score(y_true_onehot[:, i], predictions[:, i])\n",
    "        return results\n",
    "\n",
    "    def plot_confusion_matrix(self, data, class_names=None, class_indices=None):\n",
    "        \"\"\"Plot normalized confusion matrix and classification report.\"\"\"\n",
    "        predictions, y_true, _ = self.get_predictions_and_labels(data, class_indices)\n",
    "        if len(y_true) == 0:\n",
    "            print(\"No valid samples to plot confusion matrix.\")\n",
    "            return\n",
    "        y_pred = np.argmax(predictions, axis=1)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                    xticklabels=class_names if class_names else 'auto',\n",
    "                    yticklabels=class_names if class_names else 'auto')\n",
    "        plt.title('Normalized Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names if class_names else None))\n",
    "\n",
    "    def plot_roc_curves(self, data, class_names=None, class_indices=None):\n",
    "        \"\"\"Plot ROC curves for each class.\"\"\"\n",
    "        predictions, _, y_true_onehot = self.get_predictions_and_labels(data, class_indices)\n",
    "        if len(y_true_onehot) == 0:\n",
    "            print(\"No valid samples to plot ROC curves.\")\n",
    "            return\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.grid(True)\n",
    "        num_classes = len(class_indices) if class_indices is not None else self.num_classes\n",
    "        cmap = plt.cm.get_cmap('nipy_spectral', num_classes)\n",
    "        colors = cmap(np.linspace(0, 1, num_classes))\n",
    "        for i in range(num_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_onehot[:, i], predictions[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            name = class_names[i] if class_names else f'Class {i}'\n",
    "            plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "        fpr_micro, tpr_micro, _ = roc_curve(y_true_onehot.ravel(), predictions.ravel())\n",
    "        roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "        plt.plot(fpr_micro, tpr_micro, label=f'Micro-average (AUC = {roc_auc_micro:.2f})',\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "        all_fpr = np.unique(np.concatenate([\n",
    "            roc_curve(y_true_onehot[:, i], predictions[:, i])[0] for i in range(num_classes)\n",
    "        ]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(num_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_true_onehot[:, i], predictions[:, i])\n",
    "            sorted_idx = np.argsort(fpr)\n",
    "            fpr = fpr[sorted_idx]\n",
    "            tpr = tpr[sorted_idx]\n",
    "            mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
    "        mean_tpr /= num_classes\n",
    "        roc_auc_macro = auc(all_fpr, mean_tpr)\n",
    "        plt.plot(all_fpr, mean_tpr, label=f'Macro-average (AUC = {roc_auc_macro:.2f})',\n",
    "                 color='navy', linestyle='--', linewidth=4)\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random chance')\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), fontsize='small')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for determining only the Gabor filter hyperparameters\n",
    "\n",
    "# def optimise_hyperparameters(n_trials,class_weights): \n",
    "\n",
    "#     def objective(trial):\n",
    "\n",
    "#         angles_deg = np.arange(0, 180, 11.25)\n",
    "#         theta = np.radians(angles_deg)\n",
    "        \n",
    "#         hps = {\n",
    "#             'sigma': trial.suggest_float('sigma', 3, 6, step=0.2),\n",
    "#             'gamma': trial.suggest_float('gamma', 0.2, 0.8, step=0.05),\n",
    "#             'lambd': trial.suggest_float('lambd', 4, 8, step=0.2),\n",
    "#             'theta': theta.tolist(),\n",
    "#             # 'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "#             'learning_rate': 1e-3\n",
    "#         }\n",
    "\n",
    "\n",
    "#         args = [\n",
    "#             'python',\n",
    "#             'optimise_filter.py',\n",
    "#             str(hps),\n",
    "#             'Training',\n",
    "#             'Testing',\n",
    "#             str(class_weights)\n",
    "#         ]\n",
    "        \n",
    "#         gc.collect()\n",
    "#         tf.keras.backend.clear_session()\n",
    "#         time.sleep(15)\n",
    "\n",
    "#         print(\"subprocess about to start\")\n",
    "\n",
    "#         subprocess.run(args,check=True,stdout=subprocess.DEVNULL)\n",
    "\n",
    "#         with open(\"fold_metric_optuna.txt\",\"r\") as f:\n",
    "#             for line in f:\n",
    "#                 return(float(line))\n",
    "    \n",
    "\n",
    "#     study = optuna.create_study(directions=['maximize'],sampler=NSGAIISampler())\n",
    "#     study.optimize(objective, n_trials=n_trials)\n",
    "#     return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this function is for determining the bit-widths, or number of clusters or both for fixed Gabor filter hyperparameters\n",
    "\n",
    "def optimise_hyperparameters(n_trials,class_weights):\n",
    "\n",
    "    trials_data = []\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        ## angles are arranged for different configurations with equal spacing\n",
    "\n",
    "        angles_deg = np.arange(0, 180, 180/4)\n",
    "        theta = np.radians(angles_deg)\n",
    "\n",
    "        bit_widths = []\n",
    "        for i in range(12):\n",
    "            bit_widths.append(trial.suggest_int(f'bit_width{i}', 2, 6))\n",
    "        \n",
    "        bit_widths.append(2) \n",
    "        \n",
    "        for i in range(12):\n",
    "            bit_widths.append(bit_widths[12-i-1])\n",
    "\n",
    "        reshaped_bit_widths = [bit_widths[i:i+5] for i in range(0, len(bit_widths), 5)] # symmetrical filter design\n",
    "        \n",
    "        ## choose to include clusters or bit_widths based on your requirement\n",
    "        \n",
    "        # clusters = trial.suggest_int('clusters', 2, 9)\n",
    "\n",
    "        # 4\n",
    "\n",
    "        hps = {\n",
    "            'sigma': 5,\n",
    "            'gamma': 0.55,\n",
    "            'lambd': 6.0,\n",
    "            'theta': theta.tolist(),\n",
    "            'learning_rate': 1e-3\n",
    "            # ,'bit_widths': [trial.suggest_int(f'bit_width{i}', 2, 6) for i in range(clusters)]\n",
    "            # ,'clusters' : clusters\n",
    "             ,'bit_widths': reshaped_bit_widths\n",
    "        }\n",
    "\n",
    "        # 8\n",
    "\n",
    "        # hps = {\n",
    "        #     'sigma': 5.8,\n",
    "        #     'gamma': 0.65,\n",
    "        #     'lambd': 6.8,\n",
    "        #     'theta': theta.tolist(),\n",
    "        #     'learning_rate': 1e-3\n",
    "        #     # ,'bit_widths': [trial.suggest_int(f'bit_width{i}', 2, 6) for i in range(clusters)]\n",
    "        #     ,'clusters' : clusters\n",
    "        # }\n",
    "\n",
    "        # 12\n",
    "\n",
    "        # hps = {\n",
    "        #     'sigma': 5.2,\n",
    "        #     'gamma': 0.45,\n",
    "        #     'lambd': 6.2,\n",
    "        #     'theta': theta.tolist(),\n",
    "        #     'learning_rate': 1e-3\n",
    "        #     ,'bit_widths': reshaped_bit_widths\n",
    "        #     # ,'clusters' : clusters\n",
    "        # }\n",
    "\n",
    "        # 16\n",
    "\n",
    "        # hps = {\n",
    "        #     'sigma': 5.8,\n",
    "        #     'gamma': 0.5,\n",
    "        #     'lambd': 6.2,\n",
    "        #     'theta': theta.tolist(),\n",
    "        #     'learning_rate': 1e-3\n",
    "        #     ,'bit_widths': [trial.suggest_int(f'bit_width{i}', 2, 6) for i in range(clusters)]\n",
    "        #     # ,'clusters' : clusters\n",
    "        # }\n",
    "\n",
    "        args = [\n",
    "            'python',\n",
    "            'optimise_filter.py',\n",
    "            str(hps),\n",
    "            'Training',\n",
    "            'Testing',\n",
    "            str(class_weights)\n",
    "        ]\n",
    "        \n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "        time.sleep(15)\n",
    "\n",
    "        print(\"subprocess about to start\")\n",
    "\n",
    "        subprocess.run(args,check=True,stdout=subprocess.DEVNULL)\n",
    "\n",
    "        ## choose the appropriate penalty calculation based on your requirement\n",
    "\n",
    "        # penalty = sum(hps['bit_widths']) + hps['clusters']\n",
    "        # penalty = sum(hps['bit_widths'])\n",
    "        # penalty = hps['clusters']\n",
    "        penalty = sum(sum(row) for row in hps['bit_widths'])\n",
    "\n",
    "        with open(\"fold_metric_optuna.txt\",\"r\") as f:\n",
    "            for line in f:\n",
    "                accuracy = float(line)\n",
    "        \n",
    "\n",
    "        trials_data.append([accuracy, penalty, hps])\n",
    "\n",
    "        return accuracy, penalty\n",
    "    \n",
    "    def plot_pareto(trials_data):\n",
    "        # Convert the list of lists to a pandas DataFrame\n",
    "        df = pd.DataFrame(trials_data, columns=['objective_value', 'penalty', 'hyperparameters'])\n",
    "\n",
    "        # Sort the DataFrame based on score (maximize the score)\n",
    "        df_sorted = df.sort_values(by='objective_value', ascending=False)\n",
    "\n",
    "        # Find Pareto front (a score cannot improve without increasing penalty)\n",
    "        pareto_front = []\n",
    "        last_penalty = float('inf')\n",
    "        \n",
    "        for _, row in df_sorted.iterrows():\n",
    "            if row['penalty'] < last_penalty:\n",
    "                pareto_front.append(row)\n",
    "                last_penalty = row['penalty']\n",
    "\n",
    "        pareto_front = pd.DataFrame(pareto_front)\n",
    "\n",
    "        # Plot all trials (score vs penalty)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(df['objective_value'], df['penalty'], label='All Trials', color='blue', alpha=0.5)\n",
    "        \n",
    "        # Highlight the Pareto front (best trade-offs)\n",
    "        plt.scatter(pareto_front['objective_value'], pareto_front['penalty'], label='Pareto Front', color='red', marker='*', s=200)\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            plt.text(row['objective_value'], row['penalty'], str(i), fontsize=9, color='black', ha='right')\n",
    "        \n",
    "        # Labels and title\n",
    "        plt.xlabel('Objective Value (Score)')\n",
    "        plt.ylabel('Penalty')\n",
    "        plt.title('Pareto Front: Trade-off Between Score and Penalty')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    study = optuna.create_study(directions=['maximize','minimize'])\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    plot_pareto(trials_data)\n",
    "    return study.best_trials, trials_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T05:06:26.793714Z",
     "iopub.status.busy": "2025-07-03T05:06:26.793518Z",
     "iopub.status.idle": "2025-07-03T05:06:26.798781Z",
     "shell.execute_reply": "2025-07-03T05:06:26.798099Z",
     "shell.execute_reply.started": "2025-07-03T05:06:26.793699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "            print(\"GPU available:\", gpus[0])\n",
    "        except RuntimeError as e:\n",
    "            print(\"GPU Initialization Error:\", e)\n",
    "    else:\n",
    "        print(\"No GPU detected. Using CPU.\")  \n",
    "    \n",
    "\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    config = ModelConfig(img_height=512, img_width=512, channels=1, batch_size=10)\n",
    "    train_dir, validation_dir = 'Training', 'Testing'\n",
    "    if not all(map(os.path.exists, [train_dir, validation_dir])):\n",
    "        raise ValueError(\"Training or validation directory missing\")\n",
    "\n",
    "    data_processor = DataProcessor(config)\n",
    "\n",
    "    try:\n",
    "\n",
    "        train_ds, val_ds = data_processor.create_full_dataset(train_dir, validation_dir)\n",
    "        class_weights = data_processor.compute_class_weights_from_dataset(train_ds)\n",
    "\n",
    "        best_param, trials_data = optimise_hyperparameters(n_trials=100,class_weights=class_weights)\n",
    "        # best_param = optimise_hyperparameters(n_trials=100,class_weights=class_weights)\n",
    "\n",
    "        data_to_save = [\n",
    "            {\"index\": idx, \"objective_value\": trial[0], \"penalty\": trial[1], \"hyperparameters\": trial[2]} \n",
    "            for idx, trial in enumerate(trials_data)\n",
    "        ]\n",
    "        \n",
    "        with open('trials_data.json', 'w') as f:\n",
    "            json.dump(data_to_save, f, indent=4)\n",
    "\n",
    "        # print(\"best hyperparameters: \", best_param)\n",
    "\n",
    "        # with open(\"best_params.txt\", \"w\") as file:\n",
    "        #     file.write(\"Best hyperparameters:\\n\")\n",
    "        #     for key, value in best_param.items():\n",
    "        #         file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "        return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1608934,
     "sourceId": 2645886,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mypyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
